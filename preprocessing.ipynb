{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ibo192R5jbRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84d5802d-cb79-429d-9670-017b25155daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing (Tweepfake)"
      ],
      "metadata": {
        "id": "AyQ52GnJ_4vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "ABSOLUTE_PATH = \"/content/drive/My Drive/deepfake_tweets\"\n",
        "EXPORT_DIR_PATH = os.path.join(ABSOLUTE_PATH, \"data/preprocessed/\")"
      ],
      "metadata": {
        "id": "8EaLaUgW_75y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import split dataset\n",
        "train_set_path, val_set_path, test_set_path = (\n",
        "    os.path.join(ABSOLUTE_PATH, \"data/tweepfake_train.csv\"),\n",
        "    os.path.join(ABSOLUTE_PATH, \"data/tweepfake_val.csv\"),\n",
        "    os.path.join(ABSOLUTE_PATH, \"data/tweepfake_test.csv\")\n",
        ")\n",
        "\n",
        "train_df, val_df, test_df = (\n",
        "    pd.read_csv(train_set_path, sep=\";\"),\n",
        "    pd.read_csv(val_set_path, sep=\";\"),\n",
        "    pd.read_csv(test_set_path, sep=\";\")\n",
        ")\n",
        "\n",
        "# Concatenate split sets into a single DataFrame\n",
        "df = pd.concat([train_df, val_df, test_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "qRQv8PlC5nwG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "j79ZI9cF372m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(text):\n",
        "  text = re.sub(r'https?://[^\\s\\n\\r]+', '<URL>', text) # Replace links with <URL> tag\n",
        "  text = re.sub(r'#\\w+', '<HASHTAG>', text)            # Replace hashtags with <HASHTAG> tag\n",
        "  text = re.sub(r'@\\w+', '<MENTION>', text)            # Replace mentions with <MENTION> tag\n",
        "  return text\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(clean_data)\n",
        "val_df['text'] = val_df['text'].apply(clean_data)\n",
        "test_df['text'] = test_df['text'].apply(clean_data)"
      ],
      "metadata": {
        "id": "9xuwtiPc3-yy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bzpODRGh3_GX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Normalisation"
      ],
      "metadata": {
        "id": "EcJ7WLFk4nyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalise_data(text):\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "CGDd08iP4p4l"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting preprocessed dataset"
      ],
      "metadata": {
        "id": "lGcz2cehUT-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(EXPORT_DIR_PATH):\n",
        "    os.makedirs(EXPORT_DIR_PATH)\n",
        "\n",
        "train_df.to_csv(os.path.join(EXPORT_DIR_PATH, \"tweepfake_train.csv\"), index=False)\n",
        "val_df.to_csv(os.path.join(EXPORT_DIR_PATH, \"tweepfake_val.csv\"), index=False)\n",
        "test_df.to_csv(os.path.join(EXPORT_DIR_PATH, \"tweepfake_test.csv\"), index=False)\n",
        "\n",
        "print(f\"Datasets saved to {output_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMh-WBBNUXXD",
        "outputId": "bc192d02-eef8-4574-ef35-c9e9098b2e19"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets saved to /content/drive/My Drive/deepfake_tweets/data/preprocessed/\n"
          ]
        }
      ]
    }
  ]
}
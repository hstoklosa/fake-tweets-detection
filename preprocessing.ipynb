{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ibo192R5jbRm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef487631-cccb-4409-ccb3-873876b93a62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing (Tweepfake)"
      ],
      "metadata": {
        "id": "AyQ52GnJ_4vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "ABSOLUTE_PATH = \"/content/drive/My Drive/deepfake_tweets\"\n",
        "EXPORT_DIR_PATH = os.path.join(ABSOLUTE_PATH, \"data/preprocessed/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EaLaUgW_75y",
        "outputId": "307496a3-49f4-4511-bc8d-97bbf5f7c82b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import split dataset\n",
        "train_set_path, val_set_path, test_set_path = (\n",
        "    os.path.join(ABSOLUTE_PATH, \"data/tweepfake_train.csv\"),\n",
        "    os.path.join(ABSOLUTE_PATH, \"data/tweepfake_val.csv\"),\n",
        "    os.path.join(ABSOLUTE_PATH, \"data/tweepfake_test.csv\")\n",
        ")\n",
        "\n",
        "train_df, val_df, test_df = (\n",
        "    pd.read_csv(train_set_path, sep=\";\"),\n",
        "    pd.read_csv(val_set_path, sep=\";\"),\n",
        "    pd.read_csv(test_set_path, sep=\";\")\n",
        ")\n",
        "\n",
        "# Concatenate split sets into a single DataFrame\n",
        "df = pd.concat([train_df, val_df, test_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "qRQv8PlC5nwG"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "j79ZI9cF372m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(text):\n",
        "  text = re.sub(r'https?://[^\\s\\n\\r]+', '<URL>', text) # Replace links with <URL> tag\n",
        "  text = re.sub(r'#\\w+', '<HASHTAG>', text)            # Replace hashtags with <HASHTAG> tag\n",
        "  text = re.sub(r'@\\w+', '<MENTION>', text)            # Replace mentions with <MENTION> tag\n",
        "  return text\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(clean_data)\n",
        "val_df['text'] = val_df['text'].apply(clean_data)\n",
        "test_df['text'] = test_df['text'].apply(clean_data)"
      ],
      "metadata": {
        "id": "9xuwtiPc3-yy"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "BCUblD_UTpZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tk = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "\n",
        "def tokenize_data(text):\n",
        "  text = tk.tokenize(text)\n",
        "  return ' '.join(text)\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(tk.tokenize)\n",
        "val_df['text'] = val_df['text'].apply(tk.tokenize)\n",
        "test_df['text'] = test_df['text'].apply(tk.tokenize)"
      ],
      "metadata": {
        "id": "Cg8foiqzTzbL"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Normalisation"
      ],
      "metadata": {
        "id": "EcJ7WLFk4nyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalise_data(text):\n",
        "  return text"
      ],
      "metadata": {
        "id": "CGDd08iP4p4l"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting preprocessed dataset"
      ],
      "metadata": {
        "id": "lGcz2cehUT-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(EXPORT_DIR_PATH):\n",
        "    os.makedirs(EXPORT_DIR_PATH)\n",
        "\n",
        "train_df.to_csv(os.path.join(EXPORT_DIR_PATH, \"tweepfake_train.csv\"), index=False)\n",
        "val_df.to_csv(os.path.join(EXPORT_DIR_PATH, \"tweepfake_val.csv\"), index=False)\n",
        "test_df.to_csv(os.path.join(EXPORT_DIR_PATH, \"tweepfake_test.csv\"), index=False)\n",
        "\n",
        "print(f\"Datasets saved to {output_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMh-WBBNUXXD",
        "outputId": "b0c7b288-2234-4d97-bbdc-7c32b6f30154"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets saved to /content/drive/My Drive/deepfake_tweets/data/preprocessed/\n"
          ]
        }
      ]
    }
  ]
}